{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "J6ycGIaZfSLE"
            },
            "source": [
                "# Introduction\n",
                "\n",
                "This Speech Command recognition tutorial is based on the MatchboxNet model from the paper [\"MatchboxNet: 1D Time-Channel Separable Convolutional Neural Network Architecture for Speech Commands Recognition\"](https://arxiv.org/abs/2004.08531). MatchboxNet is a modified form of the QuartzNet architecture from the paper \"[QuartzNet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions](https://arxiv.org/pdf/1910.10261.pdf)\" with a modified decoder head to suit classification tasks.\n",
                "\n",
                "The notebook will follow the steps below:\n",
                "\n",
                " - Dataset preparation: Preparing Google Speech Commands dataset\n",
                "\n",
                " - Audio preprocessing (feature extraction): signal normalization, windowing, (log) spectrogram (or mel scale spectrogram, or MFCC)\n",
                "\n",
                " - Data augmentation using SpecAugment \"[SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779)\" to increase the number of data samples.\n",
                " \n",
                " - Develop a small Neural classification model that can be trained efficiently.\n",
                " \n",
                " - Model training on the Google Speech Commands dataset in NeMo.\n",
                " \n",
                " - Evaluation of error cases of the model by audibly hearing the samples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {
                "id": "I62_LJzc-p2b"
            },
            "outputs": [],
            "source": [
                "# Some utility imports\n",
                "import os\n",
                "from omegaconf import OmegaConf"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {
                "id": "K_M8wpkwd7d7"
            },
            "outputs": [],
            "source": [
                "# This is where the Google Speech Commands directory will be placed.\n",
                "# Change this if you don't want the data to be extracted in the current directory.\n",
                "# Select the version of the dataset required as well (can be 1 or 2)\n",
                "DATASET_VER = 1\n",
                "data_dir = './data/Audio_Speech_Actors_01-24'\n",
                "\n",
                "if DATASET_VER == 1:\n",
                "  MODEL_CONFIG = \"matchboxnet_3x1x64_v1.yaml\"\n",
                "else:\n",
                "  MODEL_CONFIG = \"matchboxnet_3x1x64_v2.yaml\"\n",
                "\n",
                "# if not os.path.exists(f\"./configs/{MODEL_CONFIG}\"):\n",
                "#   !wget -P ./configs/ \"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/asr/conf/matchboxnet/{MODEL_CONFIG}\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "eVsPFxJtg30p"
            },
            "source": [
                "## Prepare the path to manifest files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {
                "id": "ytTFGVe0g9wk"
            },
            "outputs": [],
            "source": [
                "# for our job\n",
                "\n",
                "dataset_path = \"./\"\n",
                "dataset_basedir = \"./\"\n",
                "\n",
                "train_dataset = os.path.join(dataset_basedir, 'train_manifest.json')\n",
                "val_dataset = os.path.join(dataset_basedir, 'validation_manifest.json')\n",
                "test_dataset = os.path.join(dataset_basedir, 'test_manifest.json')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "r-pyUBedh8f4"
            },
            "source": [
                "# Training - Preparation\n",
                "\n",
                "We will be training a MatchboxNet model from the paper [\"MatchboxNet: 1D Time-Channel Separable Convolutional Neural Network Architecture for Speech Commands Recognition\"](https://arxiv.org/abs/2004.08531). The benefit of MatchboxNet over JASPER models is that they use 1D Time-Channel Separable Convolutions, which greatly reduce the number of parameters required to obtain good model accuracy.\n",
                "\n",
                "MatchboxNet models generally follow the model definition pattern QuartzNet-[BxRXC], where B is the number of blocks, R is the number of convolutional sub-blocks, and C is the number of channels in these blocks. Each sub-block contains a 1-D masked convolution, batch normalization, ReLU, and dropout.\n",
                "\n",
                "An image of QuartzNet, the base configuration of MatchboxNet models, is provided below.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "T0sV4riijHJF"
            },
            "source": [
                "<p align=\"center\">\n",
                "  <img src=\"https://developer.nvidia.com/blog/wp-content/uploads/2020/05/quartznet-model-architecture-1-625x742.png\">\n",
                "</p>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {
                "id": "ieAPOM9thTN2"
            },
            "outputs": [],
            "source": [
                "# NeMo's \"core\" package\n",
                "import nemo\n",
                "# NeMo's ASR collection - this collections contains complete ASR models and\n",
                "# building blocks (modules) for ASR\n",
                "import nemo.collections.asr as nemo_asr"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ss9gLcDv30jI"
            },
            "source": [
                "## Model Configuration\n",
                "The MatchboxNet Model is defined in a config file which declares multiple important sections.\n",
                "\n",
                "They are:\n",
                "\n",
                "1) `model`: All arguments that will relate to the Model - preprocessors, encoder, decoder, optimizer and schedulers, datasets and any other related information\n",
                "\n",
                "2) `trainer`: Any argument to be passed to PyTorch Lightning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {
                "id": "yoVAs9h1lfci"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "name: CustomConv1DModel\n",
                        "model:\n",
                        "  sample_rate: 16000\n",
                        "  timesteps: 128\n",
                        "  repeat: 1\n",
                        "  dropout: 0.0\n",
                        "  kernel_size_factor: 1.0\n",
                        "  labels_full:\n",
                        "  - neutral\n",
                        "  - calm\n",
                        "  - happy\n",
                        "  - sad\n",
                        "  - angry\n",
                        "  - fear\n",
                        "  - disgust\n",
                        "  - surprise\n",
                        "  labels_subset:\n",
                        "  - neutral\n",
                        "  - calm\n",
                        "  - happy\n",
                        "  - sad\n",
                        "  - angry\n",
                        "  - fear\n",
                        "  - disgust\n",
                        "  - surprise\n",
                        "  labels:\n",
                        "  - neutral\n",
                        "  - calm\n",
                        "  - happy\n",
                        "  - sad\n",
                        "  - angry\n",
                        "  - fear\n",
                        "  - disgust\n",
                        "  - surprise\n",
                        "  train_ds:\n",
                        "    manifest_filepath: ./train_manifest.json\n",
                        "    sample_rate: 16000\n",
                        "    labels:\n",
                        "    - neutral\n",
                        "    - calm\n",
                        "    - happy\n",
                        "    - sad\n",
                        "    - angry\n",
                        "    - fear\n",
                        "    - disgust\n",
                        "    - surprise\n",
                        "    batch_size: 4\n",
                        "    shuffle: true\n",
                        "    num_workers: 4\n",
                        "    pin_memory: true\n",
                        "    is_tarred: false\n",
                        "    tarred_audio_filepaths: null\n",
                        "    shuffle_n: 1024\n",
                        "    bucketing_strategy: synced_randomized\n",
                        "    bucketing_batch_size: null\n",
                        "    bucketing_weights: null\n",
                        "    augmentor:\n",
                        "      shift:\n",
                        "        prob: 1\n",
                        "        min_shift_ms: -50.0\n",
                        "        max_shift_ms: 50.0\n",
                        "      white_noise:\n",
                        "        prob: 1\n",
                        "        min_level: -90\n",
                        "        max_level: -46\n",
                        "  validation_ds:\n",
                        "    manifest_filepath: ./validation_manifest.json\n",
                        "    sample_rate: 16000\n",
                        "    labels:\n",
                        "    - neutral\n",
                        "    - calm\n",
                        "    - happy\n",
                        "    - sad\n",
                        "    - angry\n",
                        "    - fear\n",
                        "    - disgust\n",
                        "    - surprise\n",
                        "    batch_size: 4\n",
                        "    shuffle: false\n",
                        "    num_workers: 4\n",
                        "    pin_memory: true\n",
                        "    val_loss_idx: 0\n",
                        "  test_ds:\n",
                        "    manifest_filepath: ./test_manifest.json\n",
                        "    sample_rate: 16000\n",
                        "    labels:\n",
                        "    - neutral\n",
                        "    - calm\n",
                        "    - happy\n",
                        "    - sad\n",
                        "    - angry\n",
                        "    - fear\n",
                        "    - disgust\n",
                        "    - surprise\n",
                        "    batch_size: 8\n",
                        "    shuffle: false\n",
                        "    num_workers: 4\n",
                        "    pin_memory: true\n",
                        "    test_loss_idx: 0\n",
                        "  preprocessor:\n",
                        "    _target_: nemo.collections.asr.modules.AudioToMFCCPreprocessor\n",
                        "    window_size: 0.025\n",
                        "    window_stride: 0.01\n",
                        "    window: hann\n",
                        "    n_mels: 64\n",
                        "    n_mfcc: 64\n",
                        "    n_fft: 512\n",
                        "  spec_augment:\n",
                        "    _target_: nemo.collections.asr.modules.SpectrogramAugmentation\n",
                        "    freq_masks: 2\n",
                        "    time_masks: 2\n",
                        "    freq_width: 15\n",
                        "    time_width: 25\n",
                        "    rect_masks: 5\n",
                        "    rect_time: 25\n",
                        "    rect_freq: 15\n",
                        "  crop_or_pad_augment:\n",
                        "    _target_: nemo.collections.asr.modules.CropOrPadSpectrogramAugmentation\n",
                        "    audio_length: 72000\n",
                        "  encoder:\n",
                        "    _target_: nemo.collections.asr.modules.ConvASREncoder\n",
                        "    feat_in: 64\n",
                        "    activation: relu\n",
                        "    conv_mask: true\n",
                        "    jasper:\n",
                        "    - filters: 256\n",
                        "      repeat: 1\n",
                        "      kernel:\n",
                        "      - 5\n",
                        "      stride:\n",
                        "      - 1\n",
                        "      dilation:\n",
                        "      - 1\n",
                        "      dropout: 0.0\n",
                        "      residual: false\n",
                        "      separable: false\n",
                        "      kernel_size_factor: 1.0\n",
                        "    - filters: 256\n",
                        "      repeat: 1\n",
                        "      kernel:\n",
                        "      - 5\n",
                        "      stride:\n",
                        "      - 1\n",
                        "      dilation:\n",
                        "      - 1\n",
                        "      dropout: 0.0\n",
                        "      residual: false\n",
                        "      separable: false\n",
                        "      kernel_size_factor: 1.0\n",
                        "    - filters: 128\n",
                        "      repeat: 1\n",
                        "      kernel:\n",
                        "      - 5\n",
                        "      stride:\n",
                        "      - 1\n",
                        "      dilation:\n",
                        "      - 1\n",
                        "      dropout: 0.2\n",
                        "      residual: false\n",
                        "      separable: false\n",
                        "      kernel_size_factor: 1.0\n",
                        "    - filters: 64\n",
                        "      repeat: 1\n",
                        "      kernel:\n",
                        "      - 5\n",
                        "      stride:\n",
                        "      - 1\n",
                        "      dilation:\n",
                        "      - 1\n",
                        "      dropout: 0.0\n",
                        "      residual: false\n",
                        "      separable: false\n",
                        "      kernel_size_factor: 1.0\n",
                        "    - filters: 128\n",
                        "      repeat: 1\n",
                        "      kernel:\n",
                        "      - 1\n",
                        "      stride:\n",
                        "      - 1\n",
                        "      dilation:\n",
                        "      - 1\n",
                        "      dropout: 0.0\n",
                        "      residual: false\n",
                        "  decoder:\n",
                        "    _target_: nemo.collections.asr.modules.ConvASRDecoderClassification\n",
                        "    feat_in: 128\n",
                        "    return_logits: true\n",
                        "    pooling_type: avg\n",
                        "  dense:\n",
                        "    _target_: nemo.collections.common.parts.Linear\n",
                        "    in_features: 128\n",
                        "    out_features: 32\n",
                        "    activation: relu\n",
                        "    dropout: 0.3\n",
                        "  final:\n",
                        "    _target_: nemo.collections.common.parts.Linear\n",
                        "    in_features: 32\n",
                        "    out_features: 8\n",
                        "    activation: softmax\n",
                        "  optim:\n",
                        "    name: adam\n",
                        "    lr: 0.005\n",
                        "    betas:\n",
                        "    - 0.9\n",
                        "    - 0.999\n",
                        "    weight_decay: 0.0001\n",
                        "    sched:\n",
                        "      name: CosineAnnealing\n",
                        "      warmup_steps: null\n",
                        "      warmup_ratio: 0.05\n",
                        "      min_lr: 1.0e-06\n",
                        "      last_epoch: -1\n",
                        "trainer:\n",
                        "  devices: 1\n",
                        "  max_epochs: 200\n",
                        "  max_steps: -1\n",
                        "  num_nodes: 1\n",
                        "  accelerator: gpu\n",
                        "  strategy: ddp\n",
                        "  accumulate_grad_batches: 1\n",
                        "  enable_checkpointing: false\n",
                        "  logger: false\n",
                        "  log_every_n_steps: 1\n",
                        "  val_check_interval: 1.0\n",
                        "  benchmark: false\n",
                        "exp_manager:\n",
                        "  exp_dir: null\n",
                        "  name: CustomConv1DModel\n",
                        "  create_tensorboard_logger: true\n",
                        "  create_checkpoint_callback: true\n",
                        "  create_wandb_logger: false\n",
                        "  wandb_logger_kwargs:\n",
                        "    name: null\n",
                        "    project: null\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "from omegaconf import OmegaConf\n",
                "from nemo.collections.asr.models import EncDecCTCModel\n",
                "\n",
                "# Import the custom perturbation class\n",
                "# import custom_perturbation\n",
                "\n",
                "config_path = f\"configs/{MODEL_CONFIG}\"\n",
                "config = OmegaConf.load(config_path)\n",
                "config = OmegaConf.to_container(config, resolve=True)\n",
                "config = OmegaConf.create(config)\n",
                "print(OmegaConf.to_yaml(config))\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {
                "id": "m2lJPR0a3qww"
            },
            "outputs": [],
            "source": [
                "# Preserve some useful parameters\n",
                "labels = config.model.labels\n",
                "sample_rate = config.model.sample_rate"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "8_pmjeed78rJ"
            },
            "source": [
                "### Setting up the datasets within the config\n",
                "\n",
                "If you'll notice, there are a few config dictionaries called `train_ds`, `validation_ds` and `test_ds`. These are configurations used to setup the Dataset and DataLoaders of the corresponding config.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {
                "id": "DIe6Qfs18MiQ"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "manifest_filepath: ./train_manifest.json\n",
                        "sample_rate: 16000\n",
                        "labels:\n",
                        "- neutral\n",
                        "- calm\n",
                        "- happy\n",
                        "- sad\n",
                        "- angry\n",
                        "- fear\n",
                        "- disgust\n",
                        "- surprise\n",
                        "batch_size: 4\n",
                        "shuffle: true\n",
                        "num_workers: 4\n",
                        "pin_memory: true\n",
                        "is_tarred: false\n",
                        "tarred_audio_filepaths: null\n",
                        "shuffle_n: 1024\n",
                        "bucketing_strategy: synced_randomized\n",
                        "bucketing_batch_size: null\n",
                        "bucketing_weights: null\n",
                        "augmentor:\n",
                        "  shift:\n",
                        "    prob: 1\n",
                        "    min_shift_ms: -50.0\n",
                        "    max_shift_ms: 50.0\n",
                        "  white_noise:\n",
                        "    prob: 1\n",
                        "    min_level: -90\n",
                        "    max_level: -46\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "print(OmegaConf.to_yaml(config.model.train_ds))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Fb01hl868Uc3"
            },
            "source": [
                "### `???` inside configs\n",
                "\n",
                "You will often notice that some configs have `???` in place of paths. This is used as a placeholder so that the user can change the value at a later time.\n",
                "\n",
                "Let's add the paths to the manifests to the config above."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {
                "id": "m181HXev8T97"
            },
            "outputs": [],
            "source": [
                "config.model.train_ds.manifest_filepath = train_dataset\n",
                "config.model.validation_ds.manifest_filepath = val_dataset\n",
                "config.model.test_ds.manifest_filepath = test_dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "pbXngoCM5IRG"
            },
            "source": [
                "## Building the PyTorch Lightning Trainer\n",
                "\n",
                "NeMo models are primarily PyTorch Lightning modules - and therefore are entirely compatible with the PyTorch Lightning ecosystem!\n",
                "\n",
                "Lets first instantiate a Trainer object!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {
                "id": "bYtvdBlG5afU"
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "import pytorch_lightning as pl"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {
                "id": "jRN18CdH51nN"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Trainer config - \n",
                        "\n",
                        "devices: 1\n",
                        "max_epochs: 200\n",
                        "max_steps: -1\n",
                        "num_nodes: 1\n",
                        "accelerator: gpu\n",
                        "strategy: ddp\n",
                        "accumulate_grad_batches: 1\n",
                        "enable_checkpointing: false\n",
                        "logger: false\n",
                        "log_every_n_steps: 1\n",
                        "val_check_interval: 1.0\n",
                        "benchmark: false\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "print(\"Trainer config - \\n\")\n",
                "print(OmegaConf.to_yaml(config.trainer))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {
                "id": "gHf6cHvm6H9b"
            },
            "outputs": [],
            "source": [
                "# Lets modify some trainer configs for this demo\n",
                "# Checks if we have GPU available and uses it\n",
                "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
                "config.trainer.devices = 1\n",
                "config.trainer.accelerator = accelerator\n",
                "\n",
                "config.trainer.max_epochs = 500\n",
                "\n",
                "# Remove distributed training flags\n",
                "config.trainer.strategy = 'auto'\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {
                "id": "UB9nr7G56G3L"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "GPU available: True (cuda), used: True\n",
                        "TPU available: False, using: 0 TPU cores\n",
                        "IPU available: False, using: 0 IPUs\n",
                        "HPU available: False, using: 0 HPUs\n",
                        "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
                    ]
                }
            ],
            "source": [
                "trainer = pl.Trainer(**config.trainer)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "2wt603Vq6sqX"
            },
            "source": [
                "## Setting up a NeMo Experiment\n",
                "\n",
                "NeMo has an experiment manager that handles logging and checkpointing for us, so let's use it ! "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {
                "id": "TfWJFg7p6Ezf"
            },
            "outputs": [],
            "source": [
                "from nemo.utils.exp_manager import exp_manager"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {
                "id": "SC-QPoW44-p2"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[NeMo I 2024-06-06 21:45:43 exp_manager:386] Experiments will be logged at /home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-06_21-39-05\n",
                        "[NeMo I 2024-06-06 21:45:43 exp_manager:825] TensorboardLogger has been set up\n"
                    ]
                }
            ],
            "source": [
                "exp_dir = exp_manager(trainer, config.get(\"exp_manager\", None))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {
                "id": "Yqi6rkNR7Dph"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-06_21-39-05'"
                        ]
                    },
                    "execution_count": 35,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# The exp_dir provides a path to the current experiment for easy access\n",
                "exp_dir = str(exp_dir)\n",
                "exp_dir"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "t0zz-vHH7Uuh"
            },
            "source": [
                "## Building the MatchboxNet Model\n",
                "\n",
                "MatchboxNet is an ASR model with a classification task - it generates one label for the entire provided audio stream. Therefore we encapsulate it inside the `EncDecClassificationModel` as follows."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {
                "id": "FRMrKhyf5vhy"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[NeMo I 2024-06-06 21:45:43 collections:301] Filtered duration for loading collection is  0.00 hours.\n",
                        "[NeMo I 2024-06-06 21:45:43 collections:302] Dataset loaded with 5329 items, total duration of  4.05 hours.\n",
                        "[NeMo I 2024-06-06 21:45:43 collections:304] # 5329 files loaded accounting to # 8 labels\n",
                        "[NeMo I 2024-06-06 21:45:43 collections:301] Filtered duration for loading collection is  0.00 hours.\n",
                        "[NeMo I 2024-06-06 21:45:43 collections:302] Dataset loaded with 1776 items, total duration of  1.34 hours.\n",
                        "[NeMo I 2024-06-06 21:45:43 collections:304] # 1776 files loaded accounting to # 8 labels\n",
                        "[NeMo I 2024-06-06 21:45:43 collections:301] Filtered duration for loading collection is  0.00 hours.\n",
                        "[NeMo I 2024-06-06 21:45:43 collections:302] Dataset loaded with 1777 items, total duration of  1.34 hours.\n",
                        "[NeMo I 2024-06-06 21:45:43 collections:304] # 1777 files loaded accounting to # 8 labels\n"
                    ]
                }
            ],
            "source": [
                "asr_model = nemo_asr.models.EncDecClassificationModel(cfg=config.model, trainer=trainer)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "jA9UND-Q_oyw"
            },
            "source": [
                "# Training a MatchboxNet Model\n",
                "\n",
                "As MatchboxNet is inherently a PyTorch Lightning Model, it can easily be trained in a single line - `trainer.fit(model)` !"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "3ngKcRFqBfIF"
            },
            "source": [
                "### Monitoring training progress\n",
                "\n",
                "Before we begin training, let's first create a Tensorboard visualization to monitor progress\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {
                "id": "sT3371CbJ8Rz"
            },
            "outputs": [],
            "source": [
                "try:\n",
                "  from google import colab\n",
                "  COLAB_ENV = True\n",
                "except (ImportError, ModuleNotFoundError):\n",
                "  COLAB_ENV = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {
                "id": "Cyfec0PDBsXa"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "To use tensorboard, please use this notebook in a Google Colab environment.\n"
                    ]
                }
            ],
            "source": [
                "# Load the TensorBoard notebook extension\n",
                "if COLAB_ENV:\n",
                "  %load_ext tensorboard\n",
                "else:\n",
                "  print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {
                "id": "4L5ymu-QBxmz"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "To use tensorboard, please use this notebook in a Google Colab environment.\n"
                    ]
                }
            ],
            "source": [
                "if COLAB_ENV:\n",
                "  %tensorboard --logdir {exp_dir}\n",
                "else:\n",
                "  print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ZApuELDIKQgC"
            },
            "source": [
                "### Training for 5 epochs\n",
                "We see below that the model begins to get modest scores on the validation set after just 5 epochs of training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {
                "id": "9xiUUJlH5KdD"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[NeMo I 2024-06-06 21:45:44 modelPT:728] Optimizer config = Adam (\n",
                        "    Parameter Group 0\n",
                        "        amsgrad: False\n",
                        "        betas: [0.9, 0.999]\n",
                        "        capturable: False\n",
                        "        differentiable: False\n",
                        "        eps: 1e-08\n",
                        "        foreach: None\n",
                        "        fused: None\n",
                        "        lr: 0.005\n",
                        "        maximize: False\n",
                        "        weight_decay: 0.0001\n",
                        "    )\n",
                        "[NeMo I 2024-06-06 21:45:44 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f6bee137940>\" \n",
                        "    will be used during training (effective maximum steps = 666500) - \n",
                        "    Parameters : \n",
                        "    (warmup_steps: null\n",
                        "    warmup_ratio: 0.05\n",
                        "    min_lr: 1.0e-06\n",
                        "    last_epoch: -1\n",
                        "    max_steps: 666500\n",
                        "    )\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  | Name              | Type                             | Params\n",
                        "-----------------------------------------------------------------------\n",
                        "0 | spec_augmentation | SpectrogramAugmentation          | 0     \n",
                        "1 | crop_or_pad       | CropOrPadSpectrogramAugmentation | 0     \n",
                        "2 | preprocessor      | AudioToMFCCPreprocessor          | 0     \n",
                        "3 | encoder           | ConvASREncoder                   | 624 K \n",
                        "4 | decoder           | ConvASRDecoderClassification     | 1.0 K \n",
                        "5 | loss              | CrossEntropyLoss                 | 0     \n",
                        "6 | _accuracy         | TopKClassificationAccuracy       | 0     \n",
                        "-----------------------------------------------------------------------\n",
                        "625 K     Trainable params\n",
                        "0         Non-trainable params\n",
                        "625 K     Total params\n",
                        "2.501     Total estimated model params size (MB)\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "504e1424b7d24dd4a24c4566df6ed522",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Sanity Checking: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f870d387264e43ce933197aa354bda29",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Training: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[NeMo I 2024-06-06 21:45:47 preemption:56] Preemption requires torch distributed to be initialized, disabling preemption\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[NeMo W 2024-06-06 21:45:48 nemo_logging:349] /home/catsunoki/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
                        "      warning_cache.warn(\n",
                        "    \n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "828b5ebb901441bf879a93a2bada1974",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 0, global step 1333: 'val_loss' reached 1.86763 (best 1.86763), saving model to '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-06_21-39-05/checkpoints/CustomConv1DModel--val_loss=1.8676-epoch=0.ckpt' as top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4a446bc5927b4861aa5aa97eb7ccf2da",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1, global step 2666: 'val_loss' reached 1.76117 (best 1.76117), saving model to '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-06_21-39-05/checkpoints/CustomConv1DModel--val_loss=1.7612-epoch=1.ckpt' as top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "b1a0b5535ea24607a2af1aa8e560685c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 2, global step 3999: 'val_loss' reached 1.74592 (best 1.74592), saving model to '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-06_21-39-05/checkpoints/CustomConv1DModel--val_loss=1.7459-epoch=2.ckpt' as top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "cc31716c0cc84995a4765e74f5fdac28",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 3, global step 5332: 'val_loss' reached 1.67682 (best 1.67682), saving model to '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-06_21-39-05/checkpoints/CustomConv1DModel--val_loss=1.6768-epoch=3.ckpt' as top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "27496c68d3394c3ebf2cdf70eaad6b07",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 4, global step 6665: 'val_loss' reached 1.61238 (best 1.61238), saving model to '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-06_21-39-05/checkpoints/CustomConv1DModel--val_loss=1.6124-epoch=4.ckpt' as top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "565fc6a18c4c412d93109e63b7c8c15d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 5, global step 7998: 'val_loss' reached 1.66456 (best 1.61238), saving model to '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-06_21-39-05/checkpoints/CustomConv1DModel--val_loss=1.6646-epoch=5.ckpt' as top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "bdb0871691e348a0a5029de5f0e49035",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 6, global step 9331: 'val_loss' reached 1.60152 (best 1.60152), saving model to '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-06_21-39-05/checkpoints/CustomConv1DModel--val_loss=1.6015-epoch=6.ckpt' as top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0f99a9f7311e4d3f94270236997de143",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 7, global step 10664: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "6b433c701c5b4437995f960c59e9186b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 8, global step 11997: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "00cb8224df7d43c7adac85c0ffce805f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 9, global step 13330: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a809911163a049d28b36d257caa666d0",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 10, global step 14663: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c3ecb1023d024e16843c6e8ddd2a8084",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 11, global step 15996: 'val_loss' reached 1.57077 (best 1.57077), saving model to '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-06_21-39-05/checkpoints/CustomConv1DModel--val_loss=1.5708-epoch=11.ckpt' as top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "b5eefa8e7dca46d782add24c47ebf089",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 12, global step 17329: 'val_loss' reached 1.51154 (best 1.51154), saving model to '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-06_21-39-05/checkpoints/CustomConv1DModel--val_loss=1.5115-epoch=12.ckpt' as top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c25067963095444a84928f485583d046",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 13, global step 18662: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "25a35485850f42839453b93c2073b4bd",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 14, global step 19995: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "49b666021cb44156a9bca454ee5d93a9",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 15, global step 21328: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a7327beb251c4bb99896f9eda52d3477",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 16, global step 22661: 'val_loss' reached 1.57791 (best 1.51154), saving model to '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-06_21-39-05/checkpoints/CustomConv1DModel--val_loss=1.5779-epoch=16.ckpt' as top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "77963f0940294ded8ac201fe4d5be636",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 17, global step 23994: 'val_loss' reached 1.52801 (best 1.51154), saving model to '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-06_21-39-05/checkpoints/CustomConv1DModel--val_loss=1.5280-epoch=17.ckpt' as top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f4d87fc095ae4dc2b1f2b07228929477",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 18, global step 25327: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f0cd529b9d16414283178d3a260aabf3",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 19, global step 26660: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ef652a684c0f49bc846cd9a338ebf392",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 20, global step 27993: 'val_loss' reached 1.53606 (best 1.51154), saving model to '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-06_21-39-05/checkpoints/CustomConv1DModel--val_loss=1.5361-epoch=20.ckpt' as top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d55dae7dfb0d4020b458942509f17185",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 21, global step 29326: 'val_loss' reached 1.51693 (best 1.51154), saving model to '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-06_21-39-05/checkpoints/CustomConv1DModel--val_loss=1.5169-epoch=21.ckpt' as top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f658d89fbe1e473d81afd9a0c45f8cb3",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 22, global step 30659: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "37e529da9e8f4c9eb42b0df2d4017a3a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 23, global step 31992: 'val_loss' reached 1.51871 (best 1.51154), saving model to '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-06_21-39-05/checkpoints/CustomConv1DModel--val_loss=1.5187-epoch=23.ckpt' as top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "107ddd6c10fb477f97b1bdacaf627ebf",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 24, global step 33325: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a578ed844062476ca4e4d3ed52952f45",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 25, global step 34658: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "b0c7e514ddd14bb497a2b6dc4cb64937",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 26, global step 35991: 'val_loss' reached 1.44079 (best 1.44079), saving model to '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-06_21-39-05/checkpoints/CustomConv1DModel--val_loss=1.4408-epoch=26.ckpt' as top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ba5a6d3aadcd44f58dee485ee19809e7",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 27, global step 37324: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3c528d0251c049d58a7e0c335e8f95f1",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 28, global step 38657: 'val_loss' reached 1.43903 (best 1.43903), saving model to '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-06_21-39-05/checkpoints/CustomConv1DModel--val_loss=1.4390-epoch=28.ckpt' as top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1a546ad560404a0fa614ed58e88eba48",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 29, global step 39990: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "71d5d5d23ec444cd971a6ba0de26c268",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 30, global step 41323: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c88afd132ed5426491361eb9e86f56c5",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 31, global step 42656: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f6b3b04b2de84bad88f4015b4e11b5f1",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 32, global step 43989: 'val_loss' reached 1.48167 (best 1.43903), saving model to '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-06_21-39-05/checkpoints/CustomConv1DModel--val_loss=1.4817-epoch=32.ckpt' as top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "2e9fced71b864fa7b04eb9727639f3c2",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 33, global step 45322: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0570919f2f4d466b942668baf7fa26a9",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 34, global step 46655: 'val_loss' reached 1.44232 (best 1.43903), saving model to '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-06_21-39-05/checkpoints/CustomConv1DModel--val_loss=1.4423-epoch=34.ckpt' as top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a8f77b28528a4371b84765c51ac8d019",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 35, global step 47988: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "67493d10881e4638a1e850f5f3a8de97",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 36, global step 49321: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3d879b0e80b04346b3358c42ce48c02b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 37, global step 50654: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "68ac77a12c9448a9988a24e16807d58c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 38, global step 51987: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e162b773b61f49c2870a5600157f5026",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 39, global step 53320: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1aa0ea30942f4e5bbc7cac8dc0bcd960",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 40, global step 54653: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "53ddac847fb340b1a414f1a2ca9f525b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 41, global step 55986: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "35b6624960a54cf085f5d839209096a1",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 42, global step 57319: 'val_loss' was not in top 3\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d0fcf7faf2df4661866498dbbbffdae9",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Validation: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 43, global step 58652: 'val_loss' was not in top 3\n"
                    ]
                }
            ],
            "source": [
                "trainer.fit(asr_model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Dkds1jSvKgSc"
            },
            "source": [
                "### Evaluation on the Test set\n",
                "\n",
                "Lets compute the final score on the test set via `trainer.test(model)`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "mULTrhEJ_6wV"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "68fad8db5782412ebb61b58d2b9b244e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Testing: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
                            "<span style=\"font-weight: bold\">        Test metric        </span><span style=\"font-weight: bold\">       DataLoader 0        </span>\n",
                            "\n",
                            "<span style=\"color: #008080; text-decoration-color: #008080\">     test_epoch_top@1      </span><span style=\"color: #800080; text-decoration-color: #800080\">    0.3055555522441864     </span>\n",
                            "<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span><span style=\"color: #800080; text-decoration-color: #800080\">     1.772859811782837     </span>\n",
                            "\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\n",
                            "\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m\n",
                            "\n",
                            "\u001b[36m \u001b[0m\u001b[36m    test_epoch_top@1     \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m   0.3055555522441864    \u001b[0m\u001b[35m \u001b[0m\n",
                            "\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m    1.772859811782837    \u001b[0m\u001b[35m \u001b[0m\n",
                            "\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "[{'test_loss': 1.772859811782837, 'test_epoch_top@1': 0.3055555522441864}]"
                        ]
                    },
                    "execution_count": 133,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "trainer.test(asr_model, ckpt_path=None)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "XQntce8cLiUC"
            },
            "source": [
                "# Fast Training\n",
                "\n",
                "We can dramatically improve the time taken to train this model by using Multi GPU training along with Mixed Precision.\n",
                "\n",
                "```python\n",
                "# Trainer with a distributed backend:\n",
                "trainer = Trainer(devices=2, num_nodes=2, accelerator='gpu', strategy='auto')\n",
                "\n",
                "# Mixed precision:\n",
                "trainer = Trainer(amp_level='O1', precision=16)\n",
                "\n",
                "# Of course, you can combine these flags as well.\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ifDHkunjM8y6"
            },
            "source": [
                "# Evaluation of incorrectly predicted samples\n",
                "\n",
                "Given that we have a trained model, which performs reasonably well, let's try to listen to the samples where the model is least confident in its predictions.\n",
                "\n",
                "For this, we need the support of the librosa library.\n",
                "\n",
                "**NOTE**: The following code depends on librosa. To install it, run the following code block first."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "s3w3LhHcKuD2"
            },
            "outputs": [],
            "source": [
                "# !pip install librosa"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "PcJrZ72sNCkM"
            },
            "source": [
                "## Extract the predictions from the model\n",
                "\n",
                "We want to possess the actual logits of the model instead of just the final evaluation score, so we can define a function to perform the forward step for us without computing the final loss. Instead, we extract the logits per batch of samples provided."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "rvxdviYtOFjK"
            },
            "source": [
                "## Accessing the data loaders\n",
                "\n",
                "We can utilize the `setup_test_data` method in order to instantiate a data loader for the dataset we want to analyze.\n",
                "\n",
                "For convenience, we can access these instantiated data loaders using the following accessors - `asr_model._train_dl`, `asr_model._validation_dl` and `asr_model._test_dl`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "CB0QZCAmM656"
            },
            "outputs": [],
            "source": [
                "asr_model.setup_test_data(config.model.test_ds)\n",
                "test_dl = asr_model._test_dl"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "rA7gXawcPoip"
            },
            "source": [
                "## Partial Test Step\n",
                "\n",
                "Below we define a utility function to perform most of the test step. For reference, the test step is defined as follows:\n",
                "\n",
                "```python\n",
                "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
                "        audio_signal, audio_signal_len, labels, labels_len = batch\n",
                "        logits = self.forward(input_signal=audio_signal, input_signal_length=audio_signal_len)\n",
                "        loss_value = self.loss(logits=logits, labels=labels)\n",
                "        correct_counts, total_counts = self._accuracy(logits=logits, labels=labels)\n",
                "        return {'test_loss': loss_value, 'test_correct_counts': correct_counts, 'test_total_counts': total_counts}\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "sBsDOm5ROpQI"
            },
            "outputs": [],
            "source": [
                "@torch.no_grad()\n",
                "def extract_logits(model, dataloader):\n",
                "  logits_buffer = []\n",
                "  label_buffer = []\n",
                "\n",
                "  # Follow the above definition of the test_step\n",
                "  for batch in dataloader:\n",
                "    audio_signal, audio_signal_len, labels, labels_len = batch\n",
                "    logits = model(input_signal=audio_signal, input_signal_length=audio_signal_len)\n",
                "\n",
                "    logits_buffer.append(logits)\n",
                "    label_buffer.append(labels)\n",
                "    print(\".\", end='')\n",
                "  print()\n",
                "  \n",
                "  print(\"Finished extracting logits !\")\n",
                "  logits = torch.cat(logits_buffer, 0)\n",
                "  labels = torch.cat(label_buffer, 0)\n",
                "  return logits, labels\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "mZSdprUlOuoV"
            },
            "outputs": [],
            "source": [
                "cpu_model = asr_model.cpu()\n",
                "cpu_model.eval()\n",
                "logits, labels = extract_logits(cpu_model, test_dl)\n",
                "print(\"Logits:\", logits.shape, \"Labels :\", labels.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "9Wd0ukgNXRBz"
            },
            "outputs": [],
            "source": [
                "# Compute accuracy - `_accuracy` is a PyTorch Lightning Metric !\n",
                "acc = cpu_model._accuracy(logits=logits, labels=labels)\n",
                "print(\"Accuracy : \", float(acc[0]*100))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "NwN9OSqCauSH"
            },
            "source": [
                "## Filtering out incorrect samples\n",
                "Let us now filter out the incorrectly labeled samples from the total set of samples in the test set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "N1YJvsmcZ0uE"
            },
            "outputs": [],
            "source": [
                "import librosa\n",
                "import json\n",
                "import IPython.display as ipd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "jZAT9yGAayvR"
            },
            "outputs": [],
            "source": [
                "# First let's create a utility class to remap the integer class labels to actual string label\n",
                "class ReverseMapLabel:\n",
                "    def __init__(self, data_loader):\n",
                "        self.label2id = dict(data_loader.dataset.label2id)\n",
                "        self.id2label = dict(data_loader.dataset.id2label)\n",
                "\n",
                "    def __call__(self, pred_idx, label_idx):\n",
                "        return self.id2label[pred_idx], self.id2label[label_idx]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "X3GSXvYHa4KJ"
            },
            "outputs": [],
            "source": [
                "# Next, let's get the indices of all the incorrectly labeled samples\n",
                "sample_idx = 0\n",
                "incorrect_preds = []\n",
                "rev_map = ReverseMapLabel(test_dl)\n",
                "\n",
                "# Remember, evaluated_tensor = (loss, logits, labels)\n",
                "probs = torch.softmax(logits, dim=-1)\n",
                "probas, preds = torch.max(probs, dim=-1)\n",
                "\n",
                "total_count = cpu_model._accuracy.total_counts_k[0]\n",
                "incorrect_ids = (preds != labels).nonzero()\n",
                "for idx in incorrect_ids:\n",
                "    proba = float(probas[idx][0])\n",
                "    pred = int(preds[idx][0])\n",
                "    label = int(labels[idx][0])\n",
                "    idx = int(idx[0]) + sample_idx\n",
                "\n",
                "    incorrect_preds.append((idx, *rev_map(pred, label), proba))\n",
                "\n",
                "print(f\"Num test samples : {total_count.item()}\")\n",
                "print(f\"Num errors : {len(incorrect_preds)}\")\n",
                "\n",
                "# First lets sort by confidence of prediction\n",
                "incorrect_preds = sorted(incorrect_preds, key=lambda x: x[-1], reverse=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "0JgGo71gcDtD"
            },
            "source": [
                "## Examine a subset of incorrect samples\n",
                "Let's print out the (test id, predicted label, ground truth label, confidence) tuple of first 20 incorrectly labeled samples\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "x37wNJsNbcw0"
            },
            "outputs": [],
            "source": [
                "for incorrect_sample in incorrect_preds[:]:\n",
                "    print(str(incorrect_sample))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "tDnwYsDKcLv9"
            },
            "source": [
                "##  Define a threshold below which we designate a model's prediction as \"low confidence\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "dpvzeh4PcGJs"
            },
            "outputs": [],
            "source": [
                "# Filter out how many such samples exist\n",
                "low_confidence_threshold = 0.25\n",
                "count_low_confidence = len(list(filter(lambda x: x[-1] <= low_confidence_threshold, incorrect_preds)))\n",
                "print(f\"Number of low confidence predictions : {count_low_confidence}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ERXyXvCAcSKR"
            },
            "source": [
                "## Let's hear the samples which the model has least confidence in !"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "kxjNVjX8cPNP"
            },
            "outputs": [],
            "source": [
                "# First let's create a helper function to parse the manifest files\n",
                "def parse_manifest(manifest):\n",
                "    data = []\n",
                "    for line in manifest:\n",
                "        line = json.loads(line)\n",
                "        data.append(line)\n",
                "\n",
                "    return data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "IWxqw5k-cUVd"
            },
            "outputs": [],
            "source": [
                "# Next, let's create a helper function to actually listen to certain samples\n",
                "def listen_to_file(sample_id, pred=None, label=None, proba=None):\n",
                "    # Load the audio waveform using librosa\n",
                "    filepath = test_samples[sample_id]['audio_filepath']\n",
                "    audio, sample_rate = librosa.load(filepath)\n",
                "\n",
                "    if pred is not None and label is not None and proba is not None:\n",
                "        print(f\"Sample : {sample_id} Prediction : {pred} Label : {label} Confidence = {proba: 0.4f}\")\n",
                "    else:\n",
                "        print(f\"Sample : {sample_id}\")\n",
                "\n",
                "    return ipd.Audio(audio, rate=sample_rate)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "HPj1tFNIcXaU"
            },
            "outputs": [],
            "source": [
                "# Now let's load the test manifest into memory\n",
                "test_samples = []\n",
                "with open(test_dataset, 'r') as test_f:\n",
                "    test_samples = test_f.readlines()\n",
                "\n",
                "test_samples = parse_manifest(test_samples)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "Nt7b_uiScZcC"
            },
            "outputs": [],
            "source": [
                "# Finally, let's listen to all the audio samples where the model made a mistake\n",
                "# Note: This list of incorrect samples may be quite large, so you may choose to subsample `incorrect_preds`\n",
                "count = min(count_low_confidence, 20)  # replace this line with just `count_low_confidence` to listen to all samples with low confidence\n",
                "\n",
                "for sample_id, pred, label, proba in incorrect_preds[:count]:\n",
                "    ipd.display(listen_to_file(sample_id, pred=pred, label=label, proba=proba))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "gxLGGDvHW2kV"
            },
            "source": [
                "# Fine-tuning on a new dataset\n",
                "\n",
                "We currently trained our dataset on all 30/35 classes of the Google Speech Commands dataset (v1/v2).\n",
                "\n",
                "We will now show an example of fine-tuning a trained model on a subset of the classes, as a demonstration of fine-tuning.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "mZAPGTzeXnuQ"
            },
            "source": [
                "## Preparing the data-subsets\n",
                "\n",
                "Let's select 2 of the classes, `yes` and `no` and prepare our manifests with this dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "G1RI4GBNfjUW"
            },
            "outputs": [],
            "source": [
                "import json"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "L3cFvN5vcbjb"
            },
            "outputs": [],
            "source": [
                "def extract_subset_from_manifest(name: str, manifest_path: str, labels: list):\n",
                "  manifest_dir = os.path.split(manifest_path)[0]\n",
                "  labels = set(labels)\n",
                "  manifest_values = []\n",
                "\n",
                "  print(f\"Parsing manifest: {manifest_path}\")\n",
                "  with open(manifest_path, 'r') as f:\n",
                "    for line in f:\n",
                "      val = json.loads(line)\n",
                "\n",
                "      if val['command'] in labels:\n",
                "        manifest_values.append(val)\n",
                "\n",
                "  print(f\"Number of files extracted from dataset: {len(manifest_values)}\")\n",
                "\n",
                "  outpath = os.path.join(manifest_dir, name)\n",
                "  with open(outpath, 'w') as f:\n",
                "    for val in manifest_values:\n",
                "      json.dump(val, f)\n",
                "      f.write(\"\\n\")\n",
                "      f.flush()\n",
                "\n",
                "  print(\"Manifest subset written to path :\", outpath)\n",
                "  print()\n",
                "\n",
                "  return outpath"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "fXQ0N1evfqZ8"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Parsing manifest: ./train_manifest.json\n"
                    ]
                },
                {
                    "ename": "KeyError",
                    "evalue": "'command'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
                        "Input \u001b[0;32mIn [95]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m train_subdataset \u001b[38;5;241m=\u001b[39m \u001b[43mextract_subset_from_manifest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_subset.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m val_subdataset \u001b[38;5;241m=\u001b[39m extract_subset_from_manifest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_subset.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, val_dataset, labels)\n\u001b[1;32m      5\u001b[0m test_subdataset \u001b[38;5;241m=\u001b[39m extract_subset_from_manifest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_subset.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_dataset, labels)\n",
                        "Input \u001b[0;32mIn [94]\u001b[0m, in \u001b[0;36mextract_subset_from_manifest\u001b[0;34m(name, manifest_path, labels)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m     val \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mval\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcommand\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;129;01min\u001b[39;00m labels:\n\u001b[1;32m     12\u001b[0m       manifest_values\u001b[38;5;241m.\u001b[39mappend(val)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of files extracted from dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(manifest_values)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
                        "\u001b[0;31mKeyError\u001b[0m: 'command'"
                    ]
                }
            ],
            "source": [
                "labels = [\"yes\", \"no\"]\n",
                "\n",
                "train_subdataset = extract_subset_from_manifest(\"train_subset.json\", train_dataset, labels)\n",
                "val_subdataset = extract_subset_from_manifest(\"val_subset.json\", val_dataset, labels)\n",
                "test_subdataset = extract_subset_from_manifest(\"test_subset.json\", test_dataset, labels)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "IO5pVNyKimiE"
            },
            "source": [
                "## Saving/Restoring a checkpoint\n",
                "\n",
                "There are multiple ways to save and load models in NeMo. Since all NeMo models are inherently Lightning Modules, we can use the standard way that PyTorch Lightning saves and restores models.\n",
                "\n",
                "NeMo also provides a more advanced model save/restore format, which encapsulates all the parts of the model that are required to restore that model for immediate use.\n",
                "\n",
                "In this example, we will explore both ways of saving and restoring models, but we will focus on the PyTorch Lightning method."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "lMKvrT88jZwC"
            },
            "source": [
                "### Saving and Restoring via PyTorch Lightning Checkpoints\n",
                "\n",
                "When using NeMo for training, it is advisable to utilize the `exp_manager` framework. It is tasked with handling checkpointing and logging (Tensorboard as well as WandB optionally!), as well as dealing with multi-node and multi-GPU logging.\n",
                "\n",
                "Since we utilized the `exp_manager` framework above, we have access to the directory where the checkpoints exist. \n",
                "\n",
                "`exp_manager` with the default settings will save multiple checkpoints for us - \n",
                "\n",
                "1) A few checkpoints from certain steps of training. They will have `--val_loss=` tags\n",
                "\n",
                "2) A checkpoint at the last epoch of training denotes by `-last`.\n",
                "\n",
                "3) If the model finishes training, it will also have a `--end` checkpoint."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "TcHTw5ErmQRi"
            },
            "outputs": [],
            "source": [
                "import glob"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "5h8zMJHngUrV"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-05_19-13-04\n"
                    ]
                }
            ],
            "source": [
                "print(exp_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "F9K_Ct_hl8oU"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-05_19-13-04/checkpoints/CustomConv1DModel--val_loss=2.0106-epoch=2.ckpt',\n",
                            " '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-05_19-13-04/checkpoints/CustomConv1DModel--val_loss=1.9699-epoch=4.ckpt',\n",
                            " '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-05_19-13-04/checkpoints/CustomConv1DModel--val_loss=1.9958-epoch=3.ckpt',\n",
                            " '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-05_19-13-04/checkpoints/CustomConv1DModel--val_loss=1.2450-epoch=123.ckpt',\n",
                            " '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-05_19-13-04/checkpoints/CustomConv1DModel--val_loss=1.2585-epoch=120.ckpt',\n",
                            " '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-05_19-13-04/checkpoints/CustomConv1DModel--val_loss=2.1162-epoch=138-last.ckpt',\n",
                            " '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-05_19-13-04/checkpoints/CustomConv1DModel--val_loss=1.2706-epoch=119.ckpt',\n",
                            " '/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-05_19-13-04/checkpoints/CustomConv1DModel--val_loss=1.9699-epoch=4-last.ckpt']"
                        ]
                    },
                    "execution_count": 98,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Let's list all the checkpoints we have\n",
                "checkpoint_dir = os.path.join(exp_dir, 'checkpoints')\n",
                "checkpoint_paths = list(glob.glob(os.path.join(checkpoint_dir, \"*.ckpt\")))\n",
                "checkpoint_paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "67fbB61umfb4"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-05_19-13-04/checkpoints/CustomConv1DModel--val_loss=2.1162-epoch=138-last.ckpt\n"
                    ]
                }
            ],
            "source": [
                "# We want the checkpoint saved after the final step of training\n",
                "final_checkpoint = list(filter(lambda x: \"-last.ckpt\" in x, checkpoint_paths))[0]\n",
                "print(final_checkpoint)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ZADUzv02nknZ"
            },
            "source": [
                "### Restoring from a PyTorch Lightning checkpoint\n",
                "\n",
                "To restore a model using the `LightningModule.load_from_checkpoint()` class method."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "ywd9Qj4Xm3VC"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[NeMo W 2024-06-05 21:01:49 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
                        "    Train config : \n",
                        "    manifest_filepath: ./train_manifest.json\n",
                        "    sample_rate: 16000\n",
                        "    labels:\n",
                        "    - neutral\n",
                        "    - calm\n",
                        "    - happy\n",
                        "    - sad\n",
                        "    - angry\n",
                        "    - fear\n",
                        "    - disgust\n",
                        "    - surprise\n",
                        "    batch_size: 4\n",
                        "    shuffle: true\n",
                        "    num_workers: 4\n",
                        "    pin_memory: true\n",
                        "    is_tarred: false\n",
                        "    tarred_audio_filepaths: null\n",
                        "    shuffle_n: 1024\n",
                        "    bucketing_strategy: synced_randomized\n",
                        "    bucketing_batch_size: null\n",
                        "    bucketing_weights: null\n",
                        "    \n",
                        "[NeMo W 2024-06-05 21:01:49 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
                        "    Validation config : \n",
                        "    manifest_filepath: ./validation_manifest.json\n",
                        "    sample_rate: 16000\n",
                        "    labels:\n",
                        "    - neutral\n",
                        "    - calm\n",
                        "    - happy\n",
                        "    - sad\n",
                        "    - angry\n",
                        "    - fear\n",
                        "    - disgust\n",
                        "    - surprise\n",
                        "    batch_size: 4\n",
                        "    shuffle: false\n",
                        "    num_workers: 4\n",
                        "    pin_memory: true\n",
                        "    val_loss_idx: 0\n",
                        "    \n",
                        "[NeMo W 2024-06-05 21:01:49 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
                        "    Test config : \n",
                        "    manifest_filepath: ./test_manifest.json\n",
                        "    sample_rate: 16000\n",
                        "    labels:\n",
                        "    - neutral\n",
                        "    - calm\n",
                        "    - happy\n",
                        "    - sad\n",
                        "    - angry\n",
                        "    - fear\n",
                        "    - disgust\n",
                        "    - surprise\n",
                        "    batch_size: 8\n",
                        "    shuffle: false\n",
                        "    num_workers: 4\n",
                        "    pin_memory: true\n",
                        "    test_loss_idx: 0\n",
                        "    \n"
                    ]
                }
            ],
            "source": [
                "restored_model = nemo_asr.models.EncDecClassificationModel.load_from_checkpoint(final_checkpoint)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "0f4GQa8vB1BB"
            },
            "source": [
                "## Prepare the model for fine-tuning\n",
                "\n",
                "Remember, the original model was trained for a 30/35 way classification task. Now we require only a subset of these models, so we need to modify the decoder head to support fewer classes.\n",
                "\n",
                "We can do this easily with the convenient function `EncDecClassificationModel.change_labels(new_label_list)`.\n",
                "\n",
                "By performing this step, we discard the old decoder head, but still, preserve the encoder!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "iMCMds7pB16U"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[NeMo I 2024-06-05 21:01:49 classification_models:703] Changed decoder output to 2 labels.\n"
                    ]
                }
            ],
            "source": [
                "restored_model.change_labels(labels)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "rrspQ2QFtbCK"
            },
            "source": [
                "### Prepare the data loaders\n",
                "\n",
                "The restored model, upon restoration, will not attempt to set up any data loaders. \n",
                "\n",
                "This is so that we can manually set up any datasets we want - train and val to finetune the model, test in order to just evaluate, or all three to do both!\n",
                "\n",
                "The entire config that we used before can still be accessed via `ModelPT.cfg`, so we will use it in order to set up our data loaders. This also gives us the opportunity to set any additional parameters we wish to setup!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# custom"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "76yDcWZ9zl2G"
            },
            "source": [
                "## Setting up a new Trainer and Experiment Manager\n",
                "\n",
                "A restored model has a utility method to attach the Trainer object to it, which is necessary in order to correctly set up the optimizer and scheduler!\n",
                "\n",
                "**Note**: The restored model does not contain the trainer config with it. It is necessary to create a new Trainer object suitable for the environment where the model is being trained. The template can be replicated from any of the training scripts.\n",
                "\n",
                "Here, since we already had the previous config object that prepared the trainer, we could have used it, but for demonstration, we will set up the trainer config manually."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "swTe3WvBzkBJ"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "devices: 1\n",
                        "accelerator: gpu\n",
                        "max_epochs: 5\n",
                        "max_steps: -1\n",
                        "num_nodes: 1\n",
                        "accumulate_grad_batches: 1\n",
                        "enable_checkpointing: false\n",
                        "logger: false\n",
                        "log_every_n_steps: 1\n",
                        "val_check_interval: 1.0\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Setup the new trainer object\n",
                "# Let's modify some trainer configs for this demo\n",
                "# Checks if we have GPU available and uses it\n",
                "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
                "\n",
                "trainer_config = OmegaConf.create(dict(\n",
                "    devices=1,\n",
                "    accelerator=accelerator,\n",
                "    max_epochs=5,\n",
                "    max_steps=-1,  # computed at runtime if not set\n",
                "    num_nodes=1,\n",
                "    accumulate_grad_batches=1,\n",
                "    enable_checkpointing=False,  # Provided by exp_manager\n",
                "    logger=False,  # Provided by exp_manager\n",
                "    log_every_n_steps=1,  # Interval of logging.\n",
                "    val_check_interval=1.0,  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations\n",
                "))\n",
                "print(OmegaConf.to_yaml(trainer_config))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "Nd_ej4bI3TIy"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "GPU available: True (cuda), used: True\n",
                        "TPU available: False, using: 0 TPU cores\n",
                        "IPU available: False, using: 0 IPUs\n",
                        "HPU available: False, using: 0 HPUs\n",
                        "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
                    ]
                }
            ],
            "source": [
                "trainer_finetune = pl.Trainer(**trainer_config)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "WtGu5q5T32XA"
            },
            "source": [
                "### Setting the trainer to the restored model\n",
                "\n",
                "All NeMo models provide a convenience method `set_trainer()` in order to setup the trainer after restoration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "BTozhedA3zpM"
            },
            "outputs": [],
            "source": [
                "restored_model.set_trainer(trainer_finetune)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "XojTpEiI3TQa"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[NeMo I 2024-06-05 21:01:54 exp_manager:386] Experiments will be logged at /home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-05_19-13-04\n",
                        "[NeMo I 2024-06-05 21:01:54 exp_manager:825] TensorboardLogger has been set up\n"
                    ]
                }
            ],
            "source": [
                "exp_dir_finetune = exp_manager(trainer_finetune, config.get(\"exp_manager\", None))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "x_LSbmCQ3TUf"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'/home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-05_19-13-04'"
                        ]
                    },
                    "execution_count": 106,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "exp_dir_finetune = str(exp_dir_finetune)\n",
                "exp_dir_finetune"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "QT_mWWnSxPLv"
            },
            "source": [
                "## Setup optimizer + scheduler\n",
                "\n",
                "For a fine-tuning experiment, let's set up the optimizer and scheduler!\n",
                "\n",
                "We will use a much lower learning rate than before, and also swap out the scheduler from PolyHoldDecay to CosineDecay."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "TugHsePsxA5Q"
            },
            "outputs": [],
            "source": [
                "import copy\n",
                "optim_sched_cfg = copy.deepcopy(restored_model.cfg.optim)\n",
                "# Struct mode prevents us from popping off elements from the config, so let's disable it\n",
                "OmegaConf.set_struct(optim_sched_cfg, False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "pZSo0sWPxwiG"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "name: adam\n",
                        "lr: 0.001\n",
                        "betas:\n",
                        "- 0.9\n",
                        "- 0.999\n",
                        "weight_decay: 0.0001\n",
                        "sched:\n",
                        "  name: CosineAnnealing\n",
                        "  warmup_steps: null\n",
                        "  warmup_ratio: 0.05\n",
                        "  min_lr: 0.0001\n",
                        "  last_epoch: -1\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Lets change the maximum learning rate to previous minimum learning rate\n",
                "optim_sched_cfg.lr = 0.001\n",
                "\n",
                "# Lets change the scheduler\n",
                "optim_sched_cfg.sched.name = \"CosineAnnealing\"\n",
                "\n",
                "# # \"power\" isn't applicable to CosineAnnealing so let's remove it\n",
                "# optim_sched_cfg.sched.pop('power')\n",
                "\n",
                "# # \"hold_ratio\" isn't applicable to CosineAnnealing, so let's remove it\n",
                "# optim_sched_cfg.sched.pop('hold_ratio')\n",
                "\n",
                "# Set \"min_lr\" to lower value\n",
                "optim_sched_cfg.sched.min_lr = 1e-4\n",
                "\n",
                "print(OmegaConf.to_yaml(optim_sched_cfg))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "FqqyFF3Ey5If"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[NeMo I 2024-06-05 21:01:57 modelPT:728] Optimizer config = Adam (\n",
                        "    Parameter Group 0\n",
                        "        amsgrad: False\n",
                        "        betas: [0.9, 0.999]\n",
                        "        capturable: False\n",
                        "        differentiable: False\n",
                        "        eps: 1e-08\n",
                        "        foreach: None\n",
                        "        fused: None\n",
                        "        lr: 0.001\n",
                        "        maximize: False\n",
                        "        weight_decay: 0.0001\n",
                        "    )\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[NeMo W 2024-06-05 21:01:57 lr_scheduler:841] As `t_max_epochs` is provided/computed, it is required to pass the train dataloader in order\n",
                        "    to compute effective maximum number of steps.\n",
                        "    Scheduler will not be instantiated !\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "(Adam (\n",
                            " Parameter Group 0\n",
                            "     amsgrad: False\n",
                            "     betas: [0.9, 0.999]\n",
                            "     capturable: False\n",
                            "     differentiable: False\n",
                            "     eps: 1e-08\n",
                            "     foreach: None\n",
                            "     fused: None\n",
                            "     lr: 0.001\n",
                            "     maximize: False\n",
                            "     weight_decay: 0.0001\n",
                            " ),\n",
                            " None)"
                        ]
                    },
                    "execution_count": 109,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Now lets update the optimizer settings\n",
                "restored_model.setup_optimization(optim_sched_cfg)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "mdivgIPUzgP_"
            },
            "outputs": [],
            "source": [
                "# We can also just directly replace the config inplace if we choose to\n",
                "restored_model.cfg.optim = optim_sched_cfg"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "3-lRyz2_Eyrl"
            },
            "source": [
                "## Fine-tune training step\n",
                "\n",
                "We fine-tune on the subset classification problem. Note, the model was originally trained on these classes (the subset defined here has already been trained on above).\n",
                "\n",
                "When fine-tuning on a truly new dataset, we will not see such a dramatic improvement in performance. However, it should still converge a little faster than if it was trained from scratch."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "nq-iHIgx6OId"
            },
            "source": [
                "### Monitor training progress via Tensorboard\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "PIacDWcD5vCR"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "To use tensorboard, please use this notebook in a Google Colab environment.\n"
                    ]
                }
            ],
            "source": [
                "if COLAB_ENV:\n",
                "  %tensorboard --logdir {exp_dir_finetune}\n",
                "else:\n",
                "  print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "r5_z1eW76fip"
            },
            "source": [
                "### Fine-tuning for 5 epochs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "WH8rN6dA6V9S"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[NeMo W 2024-06-05 21:01:59 nemo_logging:349] /home/catsunoki/anaconda3/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /home/catsunoki/pytorch/ser/nemo_experiments/CustomConv1DModel/2024-06-05_19-13-04/checkpoints exists and is not empty.\n",
                        "      rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
                        "    \n",
                        "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[NeMo I 2024-06-05 21:01:59 modelPT:728] Optimizer config = Adam (\n",
                        "    Parameter Group 0\n",
                        "        amsgrad: False\n",
                        "        betas: [0.9, 0.999]\n",
                        "        capturable: False\n",
                        "        differentiable: False\n",
                        "        eps: 1e-08\n",
                        "        foreach: None\n",
                        "        fused: None\n",
                        "        lr: 0.001\n",
                        "        maximize: False\n",
                        "        weight_decay: 0.0001\n",
                        "    )\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[NeMo W 2024-06-05 21:01:59 lr_scheduler:841] As `t_max_epochs` is provided/computed, it is required to pass the train dataloader in order\n",
                        "    to compute effective maximum number of steps.\n",
                        "    Scheduler will not be instantiated !\n",
                        "\n",
                        "  | Name              | Type                             | Params\n",
                        "-----------------------------------------------------------------------\n",
                        "0 | spec_augmentation | SpectrogramAugmentation          | 0     \n",
                        "1 | crop_or_pad       | CropOrPadSpectrogramAugmentation | 0     \n",
                        "2 | preprocessor      | AudioToMFCCPreprocessor          | 0     \n",
                        "3 | encoder           | ConvASREncoder                   | 624 K \n",
                        "4 | loss              | CrossEntropyLoss                 | 0     \n",
                        "5 | _accuracy         | TopKClassificationAccuracy       | 0     \n",
                        "6 | decoder           | ConvASRDecoderClassification     | 258   \n",
                        "-----------------------------------------------------------------------\n",
                        "624 K     Trainable params\n",
                        "0         Non-trainable params\n",
                        "624 K     Total params\n",
                        "2.498     Total estimated model params size (MB)\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "8c36d65dcde54661a5342d5870917b40",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Sanity Checking: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[NeMo W 2024-06-05 21:02:00 nemo_logging:349] /home/catsunoki/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:103: UserWarning: Total length of `list` across ranks is zero. Please make sure this was your intention.\n",
                        "      rank_zero_warn(\n",
                        "    \n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "An invalid dataloader was returned from `EncDecClassificationModel.train_dataloader()`. Found None.",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:391\u001b[0m, in \u001b[0;36m_check_dataloader_iterable\u001b[0;34m(dataloader, source, trainer_fn)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m     \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# A prefix in the message to disambiguate between the train- and (optional) val dataloader that .fit() accepts\u001b[39;00m\n",
                        "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable",
                        "\nDuring handling of the above exception, another exception occurred:\n",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "Input \u001b[0;32mIn [112]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer_finetune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrestored_model\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n\u001b[0;32m--> 532\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
                        "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[1;32m    562\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    567\u001b[0m     ckpt_path,\n\u001b[1;32m    568\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m )\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
                        "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    977\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                        "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1023\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
                        "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:194\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip:\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
                        "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:236\u001b[0m, in \u001b[0;36m_FitLoop.setup_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    234\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dl \u001b[38;5;129;01min\u001b[39;00m combined_loader\u001b[38;5;241m.\u001b[39mflattened:\n\u001b[0;32m--> 236\u001b[0m     \u001b[43m_check_dataloader_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m     dl \u001b[38;5;241m=\u001b[39m _process_dataloader(trainer, dl)\n\u001b[1;32m    238\u001b[0m     dataloaders\u001b[38;5;241m.\u001b[39mappend(dl)\n",
                        "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:407\u001b[0m, in \u001b[0;36m_check_dataloader_iterable\u001b[0;34m(dataloader, source, trainer_fn)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_overridden(source\u001b[38;5;241m.\u001b[39mname, source\u001b[38;5;241m.\u001b[39minstance):\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn invalid dataloader was passed to `Trainer.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mdataloaders=...)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataloader\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Either pass the dataloader to the `.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()` method OR implement\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `def \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(self):` in your LightningModule/LightningDataModule.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m     )\n\u001b[0;32m--> 407\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn invalid dataloader was returned from `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(source\u001b[38;5;241m.\u001b[39minstance)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataloader\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    410\u001b[0m )\n",
                        "\u001b[0;31mTypeError\u001b[0m: An invalid dataloader was returned from `EncDecClassificationModel.train_dataloader()`. Found None."
                    ]
                }
            ],
            "source": [
                "trainer_finetune.fit(restored_model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "lgV0s8auJpxV"
            },
            "source": [
                "### Evaluation on the Test set\n",
                "\n",
                "Let's compute the final score on the test set via `trainer.test(model)`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "szpLp6XTDPaK"
            },
            "outputs": [],
            "source": [
                "trainer_finetune.test(restored_model, ckpt_path=None)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "uNBAaf1FKcAZ"
            },
            "source": [
                "## Advanced Usage: Exporting a model in its entirety\n",
                "\n",
                "While most models can be easily serialized via the Experiment Manager as a PyTorch Lightning checkpoint, there are certain models where this is insufficient. \n",
                "\n",
                "Consider the case where a Model contains artifacts such as tokenizers or other intermediate file objects that cannot be so easily serialized into a checkpoint.\n",
                "\n",
                "For such cases, NeMo offers two utility functions that enable serialization of a Model + artifacts - `save_to` and `restore_from`.\n",
                "\n",
                "Further documentation regarding these methods can be obtained from the documentation pages on NeMo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "Dov9g2j8Lyjs"
            },
            "outputs": [],
            "source": [
                "import tarfile"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "WNixPPFNJyNc"
            },
            "outputs": [],
            "source": [
                "# Save a model as a tarfile\n",
                "restored_model.save_to(os.path.join(exp_dir_finetune, \"model.nemo\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "B2RHYNjjLrcW"
            },
            "outputs": [],
            "source": [
                "# The above object is just a tarfile which can store additional artifacts.\n",
                "with tarfile.open(os.path.join(exp_dir_finetune, 'model.nemo')) as blob:\n",
                "  for item in blob:\n",
                "    print(item)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "fRo04x3TLxdu"
            },
            "outputs": [],
            "source": [
                "# Restore a model from a tarfile\n",
                "restored_model_2 = nemo_asr.models.EncDecClassificationModel.restore_from(os.path.join(exp_dir_finetune, \"model.nemo\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "LyIegk2CPNsI"
            },
            "source": [
                "## Conclusion\n",
                "Once the model has been restored, either via a PyTorch Lightning checkpoint or via the `restore_from` methods, one can finetune by following the above general steps."
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "collapsed_sections": [],
            "name": "Speech_Commands.ipynb",
            "provenance": [],
            "toc_visible": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.12"
        },
        "pycharm": {
            "stem_cell": {
                "cell_type": "raw",
                "metadata": {
                    "collapsed": false
                },
                "source": []
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
